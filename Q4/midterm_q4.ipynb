{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Retrieval-Augmented Generation (RAG) System\n",
    "\n",
    "Implements a RAG pipeline using HotpotQA dataset with BM25 (sparse) and SBERT (dense) retrievers, FLAN-T5 generator, and comprehensive evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Loading and Corpus Construction\n",
    "\n",
    "Loads HotpotQA validation split and builds ~52K document corpus by flattening Wikipedia paragraphs from context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load Dataset\n",
    "print(\"Loading HotpotQA dataset (distractor, validation)...\")\n",
    "dataset = load_dataset(\"hotpot_qa\", \"distractor\", split=\"validation\")\n",
    "\n",
    "print(f\"Dataset loaded. Size: {len(dataset)}\")\n",
    "\n",
    "# 2. Create Knowledge Base (Corpus)\n",
    "# HotpotQA 'context' is a list of [title, sentences]. \n",
    "\n",
    "corpus = []\n",
    "doc_ids = []\n",
    "\n",
    "seen_titles = set()\n",
    "\n",
    "print(\"Building Corpus...\")\n",
    "for row in tqdm(dataset):\n",
    "    context = row['context'] # List of [title, sentences]\n",
    "    for title, sentences in zip(context['title'], context['sentences']):\n",
    "        if title not in seen_titles:\n",
    "            text = \" \".join(sentences)\n",
    "            corpus.append(text)\n",
    "            doc_ids.append(title)\n",
    "            seen_titles.add(title)\n",
    "\n",
    "print(f\"Corpus created. Total unique documents: {len(corpus)}\")\n",
    "print(\"Sample Document:\", corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: BM25 Sparse Retrieval\n",
    "\n",
    "Implements BM25Okapi indexing with whitespace tokenization for keyword-based document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# 1. Tokenization\n",
    "print(\"Tokenizing corpus...\")\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "# 2. Indexing\n",
    "print(\"Indexing with BM25...\")\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "print(\"Indexing complete.\")\n",
    "\n",
    "# 3. Retrieval Function\n",
    "def retrieve(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant top_k documents for the given query.\n",
    "    \"\"\"\n",
    "    tokenized_query = query.split(\" \")\n",
    "    # Get top-k documents\n",
    "    top_docs = bm25.get_top_n(tokenized_query, corpus, n=top_k)\n",
    "    return top_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: SBERT Dense Retrieval\n",
    "\n",
    "Uses `all-MiniLM-L6-v2` (384-dim) to encode corpus and perform semantic search with cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DENSE RETRIEVER (SENTENCE-BERT) SETUP ---\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import time\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"STEP 2.5: DENSE RETRIEVER (SBERT) SETUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Load Model\n",
    "# 'all-MiniLM-L6-v2': Fast and high-performance small model\n",
    "sbert_model_name = 'all-MiniLM-L6-v2'\n",
    "print(f\"Loading SBERT model ({sbert_model_name})...\")\n",
    "sbert_model = SentenceTransformer(sbert_model_name, device=device)\n",
    "\n",
    "# 2. Encode Corpus \n",
    "# Converting texts to vectors\n",
    "if 'corpus' not in locals():\n",
    "    print(\"ERROR: 'corpus' variable not found. Please run the corpus building step first.\")\n",
    "else:\n",
    "    print(\"Encoding corpus (This may take a while)...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    corpus_embeddings = sbert_model.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Encoding completed. Time: {end_time - start_time:.2f} sec\")\n",
    "    print(f\"Embedding shape: {corpus_embeddings.shape}\")\n",
    "\n",
    "# 3. Dense Retrieval Function\n",
    "def retrieve_dense(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search using SBERT.\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = sbert_model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculate Cosine Similarity\n",
    "    # Compares query vector with all corpus vectors\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    \n",
    "    # Find top_k highest scoring documents\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "    \n",
    "    # Return document texts\n",
    "    results = []\n",
    "    for idx in top_results.indices:\n",
    "        results.append(corpus[idx.item()])\n",
    "        \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: RAG Pipeline with FLAN-T5\n",
    "\n",
    "Integrates retrieval (BM25/Dense) with FLAN-T5-Base generation using context injection and beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"STEP 3: RAG PIPELINE WITH FLAN-T5 (BASE)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model_name = \"google/flan-t5-base\" \n",
    "\n",
    "print(f\"Loading model: {model_name}...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "    print(\"  FLAN-T5-Base successfully loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load base model ({e}). Please check internet connection.\")\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_and_generate(question, top_k=5, method='bm25'):\n",
    "    \"\"\"\n",
    "    Retrieves top_k documents and generates answer.\n",
    "    Returns both the answer and retrieved documents for verification.\n",
    "    \"\"\"\n",
    "    \n",
    "    # A. RETRIEVAL\n",
    "    if method == 'dense':\n",
    "        retrieved_docs = retrieve_dense(question, top_k=top_k)\n",
    "    else:\n",
    "        tokenized_query = question.lower().split(\" \")\n",
    "        retrieved_docs = bm25.get_top_n(tokenized_query, corpus, n=top_k)\n",
    "    \n",
    "    # B. GENERATION\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=50, \n",
    "        num_beams=5,       \n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer, retrieved_docs\n",
    "\n",
    "# --- TEST THE PIPELINE ---\n",
    "test_q = \"Who are the vocals of Linkin Park?\"\n",
    "print(f\"\\nTest Question: {test_q}\")\n",
    "\n",
    "# Using top_k=5 for broader context\n",
    "ans, docs = retrieve_and_generate(test_q, top_k=5, method='dense')\n",
    "\n",
    "print(f\"Model Answer: {ans}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Comprehensive Evaluation\n",
    "\n",
    "Evaluates 200 samples with retrieval metrics (Precision@k, Recall@k) and generation metrics (BLEU-1, ROUGE-L, BERTScore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "# Load metrics if not already defined\n",
    "try:\n",
    "    bleu\n",
    "except NameError:\n",
    "    print(\"Loading metrics...\")\n",
    "    bleu = evaluate.load('bleu')\n",
    "    rouge = evaluate.load('rouge')\n",
    "    bertscore = evaluate.load('bertscore')\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(len(dataset), 200, replace=False)\n",
    "comparison_samples = dataset.select(indices)\n",
    "\n",
    "print(f\"Comparison Test Set: {len(comparison_samples)} samples selected.\")\n",
    "print(f\"Retrieval Pool Size: {len(corpus)} unique documents\")\n",
    "print(f\"Test/Pool Ratio: {len(comparison_samples)/len(corpus)*200:.2f}%\")\n",
    "\n",
    "# Helper function to calculate Precision@k and Recall@k\n",
    "def calculate_retrieval_metrics(retrieved_docs, ground_truth_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Precision@k and Recall@k for retrieval.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: List of retrieved document titles (top-k)\n",
    "        ground_truth_docs: List of relevant document titles from supporting_facts\n",
    "        k: Number of top documents to consider\n",
    "        \n",
    "    Returns:\n",
    "        precision: Precision@k score\n",
    "        recall: Recall@k score\n",
    "    \"\"\"\n",
    "    retrieved_set = set(retrieved_docs[:k])\n",
    "    relevant_set = set(ground_truth_docs)\n",
    "    \n",
    "    if len(relevant_set) == 0:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    hits = len(retrieved_set.intersection(relevant_set))\n",
    "    \n",
    "    precision = hits / k if k > 0 else 0.0\n",
    "    recall = hits / len(relevant_set) if len(relevant_set) > 0 else 0.0\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "# Preparation for metrics - expanded metrics\n",
    "methods = ['bm25', 'dense']\n",
    "results = {\n",
    "    'Method': [],\n",
    "    'Precision@1': [],\n",
    "    'Precision@3': [],\n",
    "    'Precision@5': [],\n",
    "    'Recall@1': [],\n",
    "    'Recall@3': [],\n",
    "    'Recall@5': [],\n",
    "    'BLEU-1': [],\n",
    "    'ROUGE-L': [],\n",
    "    'BERTScore': []\n",
    "}\n",
    "\n",
    "print(\"\\n=== COMPREHENSIVE EVALUATION ===\")\n",
    "print(\"Evaluating Retrieval Metrics: Precision@k, Recall@k\")\n",
    "print(\"Evaluating Generation Metrics: BLEU, ROUGE-L, BERTScore\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\n--- Evaluating Method: {method.upper()} ---\")\n",
    "    \n",
    "    # Retrieval metrics storage\n",
    "    precision_at_1 = []\n",
    "    precision_at_3 = []\n",
    "    precision_at_5 = []\n",
    "    recall_at_1 = []\n",
    "    recall_at_3 = []\n",
    "    recall_at_5 = []\n",
    "    \n",
    "    # Generation metrics storage\n",
    "    predictions = []\n",
    "    references = []\n",
    "    empty_generation_count = 0  # Track empty generations\n",
    "    \n",
    "    for row in tqdm(comparison_samples, desc=f\"{method.upper()} Evaluation\"):\n",
    "        question = row['question']\n",
    "        ground_truth = row['answer']\n",
    "        \n",
    "        # Get supporting facts (ground truth relevant documents)\n",
    "        supporting_facts = row['supporting_facts']\n",
    "        relevant_doc_titles = [fact[0] for fact in zip(supporting_facts['title'], supporting_facts['sent_id'])]\n",
    "        relevant_doc_titles = list(set(relevant_doc_titles))  # Remove duplicates\n",
    "        \n",
    "        # RAG Pipeline with document titles\n",
    "        if method == 'dense':\n",
    "            retrieved_docs_texts = retrieve_dense(question, top_k=5)\n",
    "        else:\n",
    "            tokenized_query = question.lower().split(\" \")\n",
    "            retrieved_docs_texts = bm25.get_top_n(tokenized_query, corpus, n=5)\n",
    "        \n",
    "        # Map retrieved documents back to their titles for retrieval metrics\n",
    "        retrieved_titles = []\n",
    "        for doc_text in retrieved_docs_texts:\n",
    "            # Find the title by matching the document text to corpus\n",
    "            try:\n",
    "                doc_idx = corpus.index(doc_text)\n",
    "                retrieved_titles.append(doc_ids[doc_idx])\n",
    "            except ValueError:\n",
    "                pass  # Document not found in corpus\n",
    "        \n",
    "        # Calculate Precision@k and Recall@k\n",
    "        p1, r1 = calculate_retrieval_metrics(retrieved_titles, relevant_doc_titles, k=1)\n",
    "        p3, r3 = calculate_retrieval_metrics(retrieved_titles, relevant_doc_titles, k=3)\n",
    "        p5, r5 = calculate_retrieval_metrics(retrieved_titles, relevant_doc_titles, k=5)\n",
    "        \n",
    "        precision_at_1.append(p1)\n",
    "        precision_at_3.append(p3)\n",
    "        precision_at_5.append(p5)\n",
    "        recall_at_1.append(r1)\n",
    "        recall_at_3.append(r3)\n",
    "        recall_at_5.append(r5)\n",
    "        \n",
    "        # Generation part\n",
    "        context = \" \".join(retrieved_docs_texts[:5])\n",
    "        prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50, num_beams=5, early_stopping=True)\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Handle empty generations (causes BERTScore warnings)\n",
    "        if not generated or generated.strip() == \"\":\n",
    "            generated = \"unknown\"  # Default fallback answer\n",
    "            empty_generation_count += 1\n",
    "        \n",
    "        predictions.append(generated)\n",
    "        references.append(ground_truth)\n",
    "    \n",
    "    # Aggregate retrieval metrics\n",
    "    avg_precision_at_1 = np.mean(precision_at_1)\n",
    "    avg_precision_at_3 = np.mean(precision_at_3)\n",
    "    avg_precision_at_5 = np.mean(precision_at_5)\n",
    "    avg_recall_at_1 = np.mean(recall_at_1)\n",
    "    avg_recall_at_3 = np.mean(recall_at_3)\n",
    "    avg_recall_at_5 = np.mean(recall_at_5)\n",
    "    \n",
    "    # Calculate generation metrics\n",
    "    bleu1_score = bleu.compute(predictions=predictions, references=references, max_order=1)['bleu']\n",
    "    rouge_l_score = rouge.compute(predictions=predictions, references=references)['rougeL']\n",
    "    bert_score_res = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    bert_f1 = np.mean(bert_score_res['f1'])\n",
    "    \n",
    "    # Store results\n",
    "    results['Method'].append(method.upper())\n",
    "    results['Precision@1'].append(avg_precision_at_1)\n",
    "    results['Precision@3'].append(avg_precision_at_3)\n",
    "    results['Precision@5'].append(avg_precision_at_5)\n",
    "    results['Recall@1'].append(avg_recall_at_1)\n",
    "    results['Recall@3'].append(avg_recall_at_3)\n",
    "    results['Recall@5'].append(avg_recall_at_5)\n",
    "    results['BLEU-1'].append(bleu1_score)\n",
    "    results['ROUGE-L'].append(rouge_l_score)\n",
    "    results['BERTScore'].append(bert_f1)\n",
    "    \n",
    "    print(f\"\\n{method.upper()} Results:\")\n",
    "    print(f\"  Retrieval Metrics:\")\n",
    "    print(f\"    Precision@1: {avg_precision_at_1:.4f}\")\n",
    "    print(f\"    Precision@3: {avg_precision_at_3:.4f}\")\n",
    "    print(f\"    Precision@5: {avg_precision_at_5:.4f}\")\n",
    "    print(f\"    Recall@1: {avg_recall_at_1:.4f}\")\n",
    "    print(f\"    Recall@3: {avg_recall_at_3:.4f}\")\n",
    "    print(f\"    Recall@5: {avg_recall_at_5:.4f}\")\n",
    "    print(f\"  Generation Metrics:\")\n",
    "    print(f\"    BLEU-1: {bleu1_score:.4f}\")\n",
    "    print(f\"    ROUGE-L: {rouge_l_score:.4f}\")\n",
    "    print(f\"    BERTScore: {bert_f1:.4f}\")\n",
    "    print(f\"  Generation Issues:\")\n",
    "    print(f\"    Empty generations: {empty_generation_count}/{len(comparison_samples)} ({empty_generation_count/len(comparison_samples)*100:.1f}%)\")\n",
    "\n",
    "# Create comprehensive results table\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== COMPLETE EVALUATION RESULTS ===\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Visualization 1: Retrieval Metrics Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Precision@k comparison\n",
    "precision_data = df_results[['Method', 'Precision@1', 'Precision@3', 'Precision@5']]\n",
    "precision_melted = precision_data.melt(id_vars='Method', var_name='Metric', value_name='Score')\n",
    "\n",
    "sns.barplot(data=precision_melted, x='Metric', y='Score', hue='Method', palette='Set2', ax=axes[0])\n",
    "axes[0].set_title('Retrieval: Precision@k Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(0, 1.0)\n",
    "axes[0].legend(title='Method')\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container, fmt='%.3f', padding=3)\n",
    "\n",
    "# Recall@k comparison\n",
    "recall_data = df_results[['Method', 'Recall@1', 'Recall@3', 'Recall@5']]\n",
    "recall_melted = recall_data.melt(id_vars='Method', var_name='Metric', value_name='Score')\n",
    "\n",
    "sns.barplot(data=recall_melted, x='Metric', y='Score', hue='Method', palette='Set2', ax=axes[1])\n",
    "axes[1].set_title('Retrieval: Recall@k Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim(0, 1.0)\n",
    "axes[1].legend(title='Method')\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container, fmt='%.3f', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('retrieval_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n  Retrieval metrics plot saved as 'retrieval_metrics_comparison.png'\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Generation Metrics Comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "generation_data = df_results[['Method', 'BLEU-1', 'ROUGE-L', 'BERTScore']]\n",
    "generation_melted = generation_data.melt(id_vars='Method', var_name='Metric', value_name='Score')\n",
    "\n",
    "ax = sns.barplot(data=generation_melted, x='Metric', y='Score', hue='Method', palette='viridis')\n",
    "ax.set_title('Generation Quality: BLEU, ROUGE-L, BERTScore Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.0)\n",
    "plt.legend(title='Retrieval Method')\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('generation_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Generation metrics plot saved as 'generation_metrics_comparison.png'\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization 3: Joint Performance Overview\n",
    "plt.figure(figsize=(14, 8))\n",
    "all_metrics_melted = df_results.melt(id_vars='Method', var_name='Metric', value_name='Score')\n",
    "\n",
    "ax = sns.barplot(data=all_metrics_melted, x='Metric', y='Score', hue='Method', palette='coolwarm')\n",
    "ax.set_title('Complete RAG Pipeline Evaluation: All Metrics', fontsize=16, fontweight='bold')\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_xlabel('Evaluation Metrics', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Retrieval Method', fontsize=10)\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.2f', padding=2, fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rag_complete_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  Complete evaluation plot saved as 'rag_complete_evaluation.png'\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Qualitative Analysis\n",
    "\n",
    "Identifies 3 faithful generation and 3 hallucination examples to analyze RAG success/failure patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Searching for Qualitative Examples (Faithful vs Hallucination)...\")\n",
    "print(\"Looking for 3 examples of each type...\\n\")\n",
    "\n",
    "faithful_examples = []\n",
    "hallucination_examples = []\n",
    "\n",
    "analysis_samples = dataset.select(range(50, 250)) \n",
    "\n",
    "for row in tqdm(analysis_samples, desc=\"Analyzing Examples\"):\n",
    "    if len(faithful_examples) >= 3 and len(hallucination_examples) >= 3:\n",
    "        break\n",
    "        \n",
    "    question = row['question']\n",
    "    ground_truth = row['answer']\n",
    "    \n",
    "    # Generating\n",
    "    generated, docs = retrieve_and_generate(question, top_k=5, method='dense')\n",
    "    context_text = \" \".join(docs)\n",
    "    \n",
    "    # Calculating ROUGE for this single example\n",
    "    scores = rouge.compute(predictions=[generated], references=[ground_truth])\n",
    "    rouge_l = scores['rougeL']\n",
    "    \n",
    "    # Checking Faithful\n",
    "    if len(faithful_examples) < 3:\n",
    "        if rouge_l > 0.4 or (ground_truth.lower() in context_text.lower() and rouge_l > 0.2):\n",
    "            faithful_examples.append({\n",
    "                'question': question,\n",
    "                'context': context_text[:400],\n",
    "                'ground_truth': ground_truth,\n",
    "                'generated': generated,\n",
    "                'rouge_l': rouge_l\n",
    "            })\n",
    "            continue\n",
    "\n",
    "    # Checking Hallucination\n",
    "    if len(hallucination_examples) < 3:\n",
    "        if rouge_l < 0.15 and ground_truth.lower() not in context_text.lower() and len(generated) > 3:\n",
    "            hallucination_examples.append({\n",
    "                'question': question,\n",
    "                'context': context_text[:400],\n",
    "                'ground_truth': ground_truth,\n",
    "                'generated': generated,\n",
    "                'rouge_l': rouge_l\n",
    "            })\n",
    "\n",
    "# Displaying Faithful Examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ FAITHFUL GENERATION EXAMPLES (3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, example in enumerate(faithful_examples, 1):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"FAITHFUL EXAMPLE #{i}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"\\nRetrieved Context (excerpt):\")\n",
    "    print(f\"  {example['context']}...\")\n",
    "    print(f\"\\nGround Truth Answer: {example['ground_truth']}\")\n",
    "    print(f\"Generated Answer: {example['generated']}\")\n",
    "    print(f\"ROUGE-L Score: {example['rouge_l']:.3f}\")\n",
    "    \n",
    "    if example['ground_truth'].lower() in example['context'].lower():\n",
    "        print(\"✅ Verification: Ground truth FOUND in retrieved context\")\n",
    "    else:\n",
    "        print(\"⚠️  Verification: Ground truth not in excerpt (may be further in context)\")\n",
    "    \n",
    "    print(f\"Status: ✅ FAITHFUL - Model correctly used retrieved information\")\n",
    "\n",
    "# Displayinng Hallucination Examples\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"❌ HALLUCINATION / ERROR EXAMPLES (3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, example in enumerate(hallucination_examples, 1):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"HALLUCINATION EXAMPLE #{i}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"\\nRetrieved Context (excerpt):\")\n",
    "    print(f\"  {example['context']}...\")\n",
    "    print(f\"\\nGround Truth Answer: {example['ground_truth']}\")\n",
    "    print(f\"Generated Answer: {example['generated']}\")\n",
    "    print(f\"ROUGE-L Score: {example['rouge_l']:.3f}\")\n",
    "    \n",
    "    if example['ground_truth'].lower() not in example['context'].lower():\n",
    "        print(\"❌ Problem: Ground truth NOT in retrieved context → Retrieval failure\")\n",
    "    else:\n",
    "        print(\"⚠️  Problem: Context has info but generation failed → Model error\")\n",
    "    \n",
    "    print(f\"Status: ❌ HALLUCINATION/ERROR - Model failed to provide correct answer\")\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"QUALITATIVE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✅ Faithful Examples Found: {len(faithful_examples)}\")\n",
    "print(f\"   Average ROUGE-L: {np.mean([ex['rouge_l'] for ex in faithful_examples]):.3f}\")\n",
    "print(f\"\\n❌ Hallucination Examples Found: {len(hallucination_examples)}\")\n",
    "print(f\"   Average ROUGE-L: {np.mean([ex['rouge_l'] for ex in hallucination_examples]):.3f}\")\n",
    "\n",
    "print(f\"\\nROUGE-L Gap: {np.mean([ex['rouge_l'] for ex in faithful_examples]) - np.mean([ex['rouge_l'] for ex in hallucination_examples]):.3f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "207776b3f4894b6aa1bc14c8696f1cad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_32312a1512dd44d78203e403a9f9b189",
        "IPY_MODEL_a1f4388e8bea4c7a8de7003ea83c13d2",
        "IPY_MODEL_cca2f558126146cba3d01dfa50dae5b2"
       ],
       "layout": "IPY_MODEL_ebee621bf0a84f15a024bc8ba8f5a6d6"
      }
     },
     "32312a1512dd44d78203e403a9f9b189": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c5a578f22bcb4b7b87abcb1aaadf86f3",
       "placeholder": "​",
       "style": "IPY_MODEL_805a12df1d4844629f751d6238495ecf",
       "value": "Batches: 100%"
      }
     },
     "476d40f0082640f6875a93ba34d1859f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "805a12df1d4844629f751d6238495ecf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a1f4388e8bea4c7a8de7003ea83c13d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_476d40f0082640f6875a93ba34d1859f",
       "max": 2081,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_aeda37ddc4ec4a84a30d72f4aaa67225",
       "value": 2081
      }
     },
     "aeda37ddc4ec4a84a30d72f4aaa67225": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c5a578f22bcb4b7b87abcb1aaadf86f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cca2f558126146cba3d01dfa50dae5b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fbbee991c9784701838c3b5dd4cb52ad",
       "placeholder": "​",
       "style": "IPY_MODEL_e8771f18a9184d6e932cebae486cfb3a",
       "value": " 2081/2081 [02:08&lt;00:00, 28.24it/s]"
      }
     },
     "e8771f18a9184d6e932cebae486cfb3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ebee621bf0a84f15a024bc8ba8f5a6d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fbbee991c9784701838c3b5dd4cb52ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
