{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d17d77",
   "metadata": {},
   "source": [
    "# Part 5: Interpretability & Diagnostic Evaluation\n",
    "\n",
    "This notebook analyzes the trained Transformer model from Q3 using saliency maps for input importance and entropy for uncertainty estimation. We evaluate 5 failure cases to understand model limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31069b56",
   "metadata": {},
   "source": [
    "## 1. Imports & Model Architecture\n",
    "\n",
    "Define the Transformer model architecture (PositionalEncoding, TransformerModel) identical to Q3 for loading saved weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28303123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, nhead, hid_dim, n_layers, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        # 1. Encoder Projection (BERT Vectors -> Model Dim)\n",
    "        self.bert_proj = nn.Linear(768, emb_dim)\n",
    "\n",
    "        # 2. Decoder Embedding\n",
    "        self.trg_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        # 3. Components\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim, dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=n_layers,\n",
    "            num_decoder_layers=n_layers,\n",
    "            dim_feedforward=hid_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        length = trg.shape[1]\n",
    "        mask = torch.triu(torch.ones(length, length) * float('-inf'), diagonal=1)\n",
    "        return mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        # src: [Batch, Seq, 768]\n",
    "        # trg: [Batch, Seq]\n",
    "        # --- Projection & Positional Encoding ---\n",
    "        src_emb = self.pos_encoder(self.bert_proj(src))\n",
    "        trg_emb = self.pos_encoder(self.trg_embedding(trg) * math.sqrt(self.emb_dim))\n",
    "\n",
    "        # --- Transformer Forward Pass ---\n",
    "        output = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=trg_emb,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_pad_mask,\n",
    "            tgt_key_padding_mask=tgt_pad_mask,\n",
    "            memory_key_padding_mask=src_pad_mask\n",
    "        )\n",
    "\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# --- MASK GENERATION HELPER ---\n",
    "def batch_mask_factory(src_vectors, trg_tokens, device):\n",
    "    \"\"\"Generates all necessary masks for the Transformer.\"\"\"\n",
    "    # 1. Target Look-Ahead Mask (Causal)\n",
    "    sz = trg_tokens.size(1)\n",
    "    tgt_mask = torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n",
    "\n",
    "    # 2. Source Padding Mask\n",
    "    src_pad_mask = (src_vectors.abs().sum(dim=-1) == 0)\n",
    "\n",
    "    # 3. Target Padding Mask\n",
    "    tgt_pad_mask = (trg_tokens == 0)  # PAD_IDX = 0\n",
    "\n",
    "    return tgt_mask, src_pad_mask, tgt_pad_mask\n",
    "\n",
    "print(\"‚úÖ Model architecture defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9a079",
   "metadata": {},
   "source": [
    "## 2. Load Models\n",
    "\n",
    "Load DistilBERT tokenizers and encoder, initialize Transformer with Q3 hyperparameters, and load saved weights from `transformer_model_final.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n1. Loading DistilBERT for Inference Analysis...\")\n",
    "# Tokenizers\n",
    "tokenizer_src = DistilBertTokenizer.from_pretrained('distilbert-base-german-cased')\n",
    "tokenizer_trg = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "PAD_IDX = tokenizer_src.pad_token_id\n",
    "VOCAB_SIZE_TRG = tokenizer_trg.vocab_size\n",
    "\n",
    "# BERT Model\n",
    "bert_model_inference = DistilBertModel.from_pretrained('distilbert-base-german-cased').to(device)\n",
    "bert_model_inference.eval()\n",
    "\n",
    "print(\"2. Initializing Transformer Model Architecture...\")\n",
    "# Model hyperparameters (same as Q3)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 2\n",
    "HEADS = 8\n",
    "FF_DIM = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Create model\n",
    "model_trans = TransformerModel(\n",
    "    output_dim=VOCAB_SIZE_TRG,\n",
    "    emb_dim=HID_DIM,\n",
    "    nhead=HEADS,\n",
    "    hid_dim=FF_DIM,\n",
    "    n_layers=ENC_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "print(\"3. Loading Saved Transformer Model ('transformer_model_final.pt')...\")\n",
    "try:\n",
    "    checkpoint = torch.load('transformer_model_final.pt', map_location=device)\n",
    "    # Load from checkpoint format\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        model_trans.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"‚úÖ SUCCESS: Loaded from checkpoint (Epoch {checkpoint.get('epoch', '?')}, Loss: {checkpoint.get('loss', '?'):.4f})\")\n",
    "    else:\n",
    "        # Direct state_dict format\n",
    "        model_trans.load_state_dict(checkpoint)\n",
    "        print(\"‚úÖ SUCCESS: Best model weights loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è WARNING: 'transformer_model_final.pt' not found! Using random weights.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not load model. Error: {e}\")\n",
    "    \n",
    "model_trans.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42298e3",
   "metadata": {},
   "source": [
    "## 3. Analysis Function (Saliency & Entropy)\n",
    "\n",
    "`analyze_prediction()` generates translations using greedy decoding, computes gradient-based saliency maps for input importance, and calculates entropy for uncertainty estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ecb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction(sentence, model_trans, bert_model, tokenizer_src, tokenizer_trg, device, max_len=50):\n",
    "    \"\"\"Generate translation with saliency maps and entropy analysis.\"\"\"\n",
    "    model_trans.eval()\n",
    "    \n",
    "    # Get BERT embeddings\n",
    "    inputs = tokenizer_src(sentence, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bert_output = bert_model(**inputs).last_hidden_state  # [1, Seq, 768]\n",
    "    \n",
    "    # Enable gradient tracking for saliency\n",
    "    src_vectors = bert_output.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Decoding loop\n",
    "    bos_token_id = tokenizer_trg.cls_token_id\n",
    "    eos_token_id = tokenizer_trg.sep_token_id\n",
    "    \n",
    "    trg_indexes = [bos_token_id]\n",
    "    generated_tokens = []\n",
    "    uncertainties = []\n",
    "    recent_tokens = []\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes]).to(device)\n",
    "        \n",
    "        # Create masks\n",
    "        t_mask, s_pad, t_pad = batch_mask_factory(src_vectors, trg_tensor, device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.enable_grad():\n",
    "            output = model_trans(src_vectors, trg_tensor, tgt_mask=t_mask, src_pad_mask=s_pad, tgt_pad_mask=t_pad)\n",
    "        \n",
    "        # Get logits for last token\n",
    "        last_token_logits = output[0, -1, :].clone()\n",
    "        \n",
    "        # Repetition penalty\n",
    "        for prev_token in recent_tokens[-3:]:\n",
    "            last_token_logits[prev_token] -= 2.0\n",
    "        \n",
    "        # Uncertainty (Entropy)\n",
    "        probs = F.softmax(last_token_logits, dim=0)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-9)).item()\n",
    "        uncertainties.append(entropy)\n",
    "        \n",
    "        # Greedy prediction\n",
    "        pred_token_id = last_token_logits.argmax().item()\n",
    "        \n",
    "        # Stop if same token repeated 3 times\n",
    "        if len(recent_tokens) >= 2 and all(t == pred_token_id for t in recent_tokens[-2:]):\n",
    "            break\n",
    "            \n",
    "        trg_indexes.append(pred_token_id)\n",
    "        recent_tokens.append(pred_token_id)\n",
    "        \n",
    "        token_str = tokenizer_trg.decode([pred_token_id])\n",
    "        generated_tokens.append(token_str)\n",
    "        \n",
    "        # Stop at EOS\n",
    "        if pred_token_id == eos_token_id or pred_token_id == tokenizer_trg.pad_token_id:\n",
    "            break\n",
    "    \n",
    "    # Compute saliency map via gradient backprop\n",
    "    if len(generated_tokens) > 0:\n",
    "        final_output = model_trans(src_vectors, torch.LongTensor([trg_indexes]).to(device), \n",
    "                                    tgt_mask=batch_mask_factory(src_vectors, torch.LongTensor([trg_indexes]).to(device), device)[0],\n",
    "                                    src_pad_mask=batch_mask_factory(src_vectors, torch.LongTensor([trg_indexes]).to(device), device)[1],\n",
    "                                    tgt_pad_mask=batch_mask_factory(src_vectors, torch.LongTensor([trg_indexes]).to(device), device)[2])\n",
    "        score = final_output[0, -1, :].max()\n",
    "        score.backward()\n",
    "        \n",
    "        saliency = src_vectors.grad.data.norm(dim=2).squeeze().cpu().numpy()\n",
    "        saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-9)\n",
    "    else:\n",
    "        saliency = np.zeros(src_vectors.shape[1])\n",
    "    \n",
    "    src_tokens_str = tokenizer_src.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Remove [SEP] from output\n",
    "    if generated_tokens and generated_tokens[-1] == '[SEP]':\n",
    "        generated_tokens = generated_tokens[:-1]\n",
    "        uncertainties = uncertainties[:-1]\n",
    "    \n",
    "    return \" \".join(generated_tokens), src_tokens_str, saliency, generated_tokens, uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9e4e6f",
   "metadata": {},
   "source": [
    "## 4. Visualization Helper\n",
    "\n",
    "`plot_analysis()` creates two plots: a saliency bar chart showing input token importance and an entropy line plot showing model uncertainty per generated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaba4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_analysis(src_toks, saliency, gen_toks, entropies, title):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    # Saliency Map\n",
    "    sns.barplot(x=src_toks, y=saliency, hue=src_toks, ax=ax1, palette=\"viridis\", legend=False)\n",
    "    ax1.set_title(f\"Input Importance (Saliency) - '{title}'\")\n",
    "    ax1.set_ylabel(\"Impact Score\")\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Uncertainty (Entropy)\n",
    "    ax2.plot(gen_toks, entropies, marker='o', linestyle='-', color='crimson', linewidth=2)\n",
    "    ax2.set_title(\"Model Uncertainty (Entropy) per Step\")\n",
    "    ax2.set_ylabel(\"Entropy (Higher = More Uncertain)\")\n",
    "    ax2.set_xlabel(\"Generated Output Tokens\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300f196",
   "metadata": {},
   "source": [
    "## 5. Diagnostic Evaluation: Test Cases & Results\n",
    "\n",
    "Execute 6 test cases (1 success baseline + 5 failure cases) to identify model weaknesses: ambiguity, negation loss, idiom handling, rare words (OOV), and long-range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXECUTING PART 5: DIAGNOSTIC EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test cases: 1 success baseline + 5 failure cases\n",
    "test_cases = [\n",
    "    # Success baseline\n",
    "    (\"Ein Hund l√§uft.\", \"Success Case (Simple)\"),\n",
    "    \n",
    "    # 5 Failure cases\n",
    "    (\"Das Schloss ist sehr alt.\", \"Failure 1: Ambiguity (Schloss: Lock vs Castle)\"),\n",
    "    (\"Die Kinder spielen nicht im Garten.\", \"Failure 2: Negation Loss\"),\n",
    "    (\"Das ist nicht mein Bier.\", \"Failure 3: Idiom (Literal Translation)\"),\n",
    "    (\"Der Wissenschaftler analysiert die Molekularstruktur.\", \"Failure 4: Rare Words (OOV)\"),\n",
    "    (\"Der Mann, der den Hund hat, der braun ist, geht.\", \"Failure 5: Long Dependency\")\n",
    "]\n",
    "\n",
    "# Expected translations for comparison\n",
    "expected_translations = {\n",
    "    \"Ein Hund l√§uft.\": \"A dog is running.\",\n",
    "    \"Das Schloss ist sehr alt.\": \"The castle/lock is very old.\",\n",
    "    \"Die Kinder spielen nicht im Garten.\": \"The children are not playing in the garden.\",\n",
    "    \"Das ist nicht mein Bier.\": \"That's not my problem. (Idiom)\",\n",
    "    \"Der Wissenschaftler analysiert die Molekularstruktur.\": \"The scientist analyzes the molecular structure.\",\n",
    "    \"Der Mann, der den Hund hat, der braun ist, geht.\": \"The man who has the dog that is brown is walking.\"\n",
    "}\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for sentence, case_type in test_cases:\n",
    "    print(f\"\\nProcessing: {case_type}...\")\n",
    "    pred_text, src_toks, saliency, gen_toks, entropies = analyze_prediction(\n",
    "        sentence, model_trans, bert_model_inference, tokenizer_src, tokenizer_trg, device\n",
    "    )\n",
    "    \n",
    "    expected = expected_translations.get(sentence, \"N/A\")\n",
    "    print(f\"Input: {sentence}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Prediction: {pred_text}\")\n",
    "    \n",
    "    if len(gen_toks) > 0:\n",
    "        plot_analysis(src_toks, saliency, gen_toks, entropies, case_type)\n",
    "        \n",
    "    results_table.append({\n",
    "        \"Case Type\": case_type,\n",
    "        \"Input\": sentence,\n",
    "        \"Expected\": expected,\n",
    "        \"Prediction\": pred_text,\n",
    "        \"Avg Entropy\": np.mean(entropies) if entropies else 0.0\n",
    "    })\n",
    "\n",
    "# --- 5. RESULTS TABLE & ANALYSIS ---\n",
    "df_results = pd.DataFrame(results_table)\n",
    "\n",
    "# Evaluate translation quality\n",
    "def evaluate_translation(row):\n",
    "    pred = row['Prediction'].lower().strip()\n",
    "    case_type = row['Case Type'].lower()\n",
    "    \n",
    "    if \"success\" in case_type:\n",
    "        return \"‚úÖ SUCCESS\"\n",
    "    if \"ambiguity\" in case_type and \"castle\" not in pred and \"lock\" not in pred:\n",
    "        return \"‚ùå FAILURE (Generic Fallback)\"\n",
    "    if \"negation\" in case_type and \"not\" not in pred and \"n't\" not in pred:\n",
    "        return \"‚ùå FAILURE (Negation Lost)\"\n",
    "    if \"idiom\" in case_type and (\"beer\" in pred or \"bier\" in pred):\n",
    "        return \"‚ùå FAILURE (Literal Translation)\"\n",
    "    if (\"rare\" in case_type or \"oov\" in case_type) and (\"scientist\" not in pred or \"molecular\" not in pred):\n",
    "        return \"‚ùå FAILURE (OOV Words)\"\n",
    "    if (\"long\" in case_type or \"dependency\" in case_type) and (\"who\" not in pred or \"dog\" not in pred or \"brown\" not in pred):\n",
    "        return \"‚ùå FAILURE (Structure Lost)\"\n",
    "    if row['Avg Entropy'] > 3.0:\n",
    "        return \"‚ö†Ô∏è HIGH UNCERTAINTY\"\n",
    "    return \"‚úÖ SUCCESS\"\n",
    "\n",
    "df_results['Status'] = df_results.apply(evaluate_translation, axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä DIAGNOSTIC EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "display(df_results[['Case Type', 'Input', 'Prediction', 'Avg Entropy', 'Status']])\n",
    "\n",
    "# --- 6. FAILURE ANALYSIS ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç FAILURE CASE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# --- 7. ENTROPY ANALYSIS PLOT ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors = ['green' if 'SUCCESS' in s else 'orange' if 'UNCERTAINTY' in s else 'red' \n",
    "          for s in df_results['Status']]\n",
    "bars = plt.bar(range(len(df_results)), df_results['Avg Entropy'], color=colors)\n",
    "plt.xticks(range(len(df_results)), [c.split('(')[0].strip() for c in df_results['Case Type']], rotation=45, ha='right')\n",
    "plt.ylabel('Average Entropy')\n",
    "plt.title('Model Uncertainty vs Translation Quality')\n",
    "plt.axhline(y=2.0, color='orange', linestyle='--', label='Uncertainty Threshold')\n",
    "plt.axhline(y=3.0, color='red', linestyle='--', label='High Uncertainty')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('uncertainty_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Uncertainty analysis saved: uncertainty_analysis.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
