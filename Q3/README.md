# Question 3: Transition from Recurrence to Self-Attention

## ğŸ“‹ Overview

This project analyzes the **empirical and conceptual transition** from recurrent neural networks (RNN) to self-attention-driven architectures (Transformer) for neural machine translation. The implementation uses the **Multi30k German-English** dataset and compares both architectures under fair experimental conditions.

## ğŸ¯ Objectives

- Re-implement a Seq2Seq model with **Bahdanau (Additive) Attention**
- Train a **Transformer** model on the identical dataset
- Incorporate **DistilBERT embeddings** for consistent feature extraction
- Evaluate models using **BLEU, ROUGE-L, training stability, and computational efficiency**
- Conduct **ablation studies** on layer depth and attention heads
- Analyze how self-attention enables **global dependency modeling** and **parallelization**

## ğŸ—ï¸ Architecture

### 1. RNN Seq2Seq with Bahdanau Attention

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ENCODER                               â”‚
â”‚  [768-dim BERT Vectors] â†’ Linear(768â†’256) â†’ Bidirectional GRUâ”‚
â”‚  Output: [Batch, Seq, 512] + Hidden States                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BAHDANAU ATTENTION                         â”‚
â”‚  energy = tanh(W[hidden; encoder_outputs])                   â”‚
â”‚  attention = softmax(V Ã— energy)                             â”‚
â”‚  context = Î£(attention Ã— encoder_outputs)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DECODER                               â”‚
â”‚  GRU with attention-weighted context vectors                 â”‚
â”‚  Output: [Batch, Seq, Vocab_Size]                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Transformer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ENCODER                                  â”‚
â”‚  [768-dim BERT] â†’ Linear(768â†’256) â†’ Positional Encoding     â”‚
â”‚  â†’ N Ã— (Multi-Head Self-Attention + FFN + LayerNorm)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DECODER                                  â”‚
â”‚  Token Embedding â†’ Positional Encoding                       â”‚
â”‚  â†’ N Ã— (Masked Self-Attention + Cross-Attention + FFN)      â”‚
â”‚  â†’ Linear(256 â†’ Vocab_Size)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
Q3/
â”œâ”€â”€ midterm_q3.ipynb          # Main notebook with all experiments
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ rnn_model_final.pt     # Trained RNN checkpoint
â”œâ”€â”€ transformer_model_final.pt  # Trained Transformer checkpoint
â”œâ”€â”€ ablation_models/       # Ablation study checkpoints
â”‚   â”œâ”€â”€ transformer_light.pt   # 1 layer, 4 heads
â”‚   â””â”€â”€ transformer_deep.pt    # 4 layers, 8 heads
â””â”€â”€ *.png                  # Generated visualization plots
```

## âš™ï¸ Hyperparameters

| Parameter | RNN | Transformer | Notes |
|-----------|-----|-------------|-------|
| Hidden Dimension | 256 | 256 | Same for fair comparison |
| Encoder Layers | 2 | 2 | Bidirectional for RNN |
| Decoder Layers | 2 | 2 | - |
| Attention Heads | - | 8 | Multi-head attention |
| Feed-Forward Dim | - | 512 | 2Ã— hidden dim |
| Dropout | 0.1 | 0.1 | Same for both |
| Learning Rate | 0.0005 | 0.0005 | Adam optimizer |
| Batch Size | 64 | 64 | - |
| Epochs | 5 | 5 | - |

## ğŸš€ Installation

```bash
# Clone or navigate to the project directory
cd Q3

# Install dependencies
pip install -r requirements.txt
```

### Requirements

- Python 3.8+
- PyTorch 2.0+
- Transformers (HuggingFace)
- datasets
- sacrebleu
- rouge-score
- matplotlib
- tqdm
- numpy

## ğŸ“Š Evaluation Metrics

### Translation Quality
- **BLEU Score**: N-gram precision with brevity penalty
- **ROUGE-L**: Longest common subsequence F-measure

### Computational Efficiency
- **Inference Time**: Total time to translate test set
- **GPU Memory**: Peak memory usage during inference
- **Parameter Count**: Total trainable parameters

### Training Stability
- **Loss Variance**: Standard deviation of training loss
- **Generalization Gap**: Difference between validation and training loss

## ğŸ”¬ Ablation Study

Three Transformer configurations are tested:

| Config | Layers | Heads | Purpose |
|--------|--------|-------|---------|
| **Light** | 1 | 4 | Speed optimization, fewer parameters |
| **Base** | 2 | 8 | Standard configuration |
| **Deep** | 4 | 8 | Higher capacity, deeper learning |

### Attention Mechanism

**Bahdanau (Additive) Attention:**
```
score(s_t, h_i) = v^T Ã— tanh(W_a[s_t; h_i])
```

**Scaled Dot-Product Attention:**
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V
```

## ğŸ–¼ï¸ Generated Visualizations


1. `training_stability_comparisonreal.png` - Perplexity trends
2. `training_Stability_metrics.png` - Stability bar charts
3. `ablation_stabilitypng.png` - Ablation PPL trends
4. `final_evaluation_metrics.png` - BLEU, ROUGE, Time, Memory

## ğŸ”§ Usage

### Running the Notebook

1. Open `midterm_q3.ipynb` in Jupyter/Colab/VS Code
2. Run cells sequentially (1.1 â†’ 6.1)
3. Models will be saved automatically after training

### Loading Trained Models

```python
import torch

# Load RNN Model
checkpoint = torch.load('rnn_model_final.pt')
model_rnn.load_state_dict(checkpoint['model_state_dict'])

# Load Transformer Model
checkpoint = torch.load('transformer_model_final.pt')
model_trans.load_state_dict(checkpoint['model_state_dict'])
```
