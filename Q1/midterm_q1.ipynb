{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxB2HbG4jf_F"
   },
   "source": [
    "# Question 1: Sentiment Analysis on IMDb Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "This cell sets up the entire environment for the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyWB8-Y_jf_I",
    "outputId": "3ee7396c-c6aa-4e9b-db70-8ce710f07cb7"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Hyperparameters\n",
    "EMB_DIM = 300\n",
    "HID_DIM = 256\n",
    "BATCH_SIZE = 32\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "N_EPOCHS = 5\n",
    "N_LAYERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiLfAPPujf_J"
   },
   "source": [
    "## Data Pipeline (GloVe Path)\n",
    "\n",
    "This cell implements the complete data preprocessing pipeline for GloVe-based models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f7adfabf29c448b888777e3c03eebfec"
     ]
    },
    "id": "kwxoiJumjf_J",
    "outputId": "85aa565a-90c6-42ec-fb59-7ae9b204997c"
   },
   "outputs": [],
   "source": [
    "# Import data processing utilities\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "\n",
    "# Load IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "train_val_split = dataset['train'].train_test_split(test_size=0.1, seed=SEED)\n",
    "dataset['train'] = train_val_split['train']\n",
    "dataset['validation'] = train_val_split['test']\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans text by lowercasing and expanding contractions.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"<br />\", \" \").replace(\"<br>\", \" \")\n",
    "\n",
    "    contractions = {\n",
    "        \"n't\": \" not\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'m\": \" am\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'d\": \" would\",\n",
    "    }\n",
    "\n",
    "    for c, full in contractions.items():\n",
    "        text = text.replace(c, full)\n",
    "\n",
    "    return text\n",
    "\n",
    "def glove_tokenizer(text):\n",
    "    \"\"\"Tokenizes text for GloVe embedding compatibility.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    tokens = re.findall(r\"[a-z]+\", text)\n",
    "    return tokens\n",
    "\n",
    "# Build vocabulary from training data\n",
    "print(\"Building vocabulary...\")\n",
    "counter = Counter()\n",
    "\n",
    "for text in tqdm(dataset['train']['text'], desc=\"Tokenizing Train Data\"):\n",
    "    counter.update(glove_tokenizer(text))\n",
    "\n",
    "# Special tokens\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "vocab = {'<pad>': PAD_IDX, '<unk>': UNK_IDX}\n",
    "\n",
    "# Limit vocabulary size\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "for word, freq in counter.most_common(MAX_VOCAB_SIZE):\n",
    "    if word not in vocab:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "def text_pipeline(text):\n",
    "    \"\"\"Converts text to token indices.\"\"\"\n",
    "    return [vocab.get(token, UNK_IDX) for token in glove_tokenizer(text)]\n",
    "\n",
    "MAX_LEN = 256\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"Custom collate function for DataLoader with padding.\"\"\"\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "\n",
    "    for item in batch:\n",
    "        label_list.append(float(item['label']))\n",
    "\n",
    "        processed_list = text_pipeline(item['text'])\n",
    "\n",
    "        # Truncate if exceeds max length\n",
    "        if len(processed_list) > MAX_LEN:\n",
    "            processed_list = processed_list[:MAX_LEN]\n",
    "\n",
    "        processed_text = torch.tensor(processed_list, dtype=torch.long)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "\n",
    "    # Pad sequences to same length\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.float).to(device)\n",
    "    text_list = text_list.to(device)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long).cpu()\n",
    "\n",
    "    return text_list, label_list, lengths\n",
    "\n",
    "# Create DataLoaders for GloVe path\n",
    "train_loader_glove = DataLoader(dataset['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader_glove = DataLoader(dataset['validation'], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader_glove = DataLoader(dataset['test'], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "print(\"Data pipeline ready (with Advanced Cleaning).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVP4xaTzjf_K"
   },
   "source": [
    "## GloVe Integration with Smart Caching\n",
    "\n",
    "This cell implements an efficient GloVe embedding integration system:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lk04bcE3jf_K",
    "outputId": "873d0fba-a8d5-4236-c5b1-f1fe09720621"
   },
   "outputs": [],
   "source": [
    "# Import utilities for GloVe download and processing\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# GloVe configuration\n",
    "CACHE_DIR = '.vector_cache'\n",
    "GLOVE_ZIP_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "GLOVE_FILE = 'glove.6B.300d.txt'\n",
    "\n",
    "if not os.path.exists(CACHE_DIR):\n",
    "    os.makedirs(CACHE_DIR)\n",
    "\n",
    "def download_and_extract_glove(cache_dir=CACHE_DIR):\n",
    "    \"\"\"Downloads and extracts GloVe vectors if not already cached.\"\"\"\n",
    "    target_path = os.path.join(cache_dir, GLOVE_FILE)\n",
    "    zip_path = os.path.join(cache_dir, 'glove.6B.zip')\n",
    "\n",
    "    if not os.path.exists(target_path):\n",
    "        print(f\"GloVe (300d) not found. Downloading original ZIP... ({GLOVE_ZIP_URL})\")\n",
    "        print(\"This may take 1-2 minutes depending on internet speed (862MB)...\")\n",
    "\n",
    "        urllib.request.urlretrieve(GLOVE_ZIP_URL, zip_path)\n",
    "\n",
    "        print(\"Extracting only 300d vectors from the zip...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extract(GLOVE_FILE, cache_dir)\n",
    "\n",
    "        print(\"Cleaning up (deleting Zip)...\")\n",
    "        os.remove(zip_path)\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(f\"GloVe vectors ready at {target_path}.\")\n",
    "\n",
    "    return target_path\n",
    "\n",
    "def create_embedding_matrix(vocab, emb_dim=300):\n",
    "    \"\"\"Creates embedding matrix by matching vocabulary with GloVe vectors.\"\"\"\n",
    "    glove_path = download_and_extract_glove()\n",
    "\n",
    "    print(\"Creating embedding matrix...\")\n",
    "    embeddings_index = {}\n",
    "\n",
    "    # Load GloVe vectors\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                if coefs.shape[0] == emb_dim:\n",
    "                    embeddings_index[word] = coefs\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Initialize embedding matrix\n",
    "    embedding_matrix = np.zeros((len(vocab), emb_dim))\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    missed_words = []\n",
    "\n",
    "    # Fill embedding matrix with GloVe vectors\n",
    "    for word, i in vocab.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            # Random initialization for OOV words\n",
    "            if word != '<pad>':\n",
    "                embedding_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "            misses += 1\n",
    "            missed_words.append(word)\n",
    "\n",
    "    print(f\"Embedding Matrix Ready. Shape: {embedding_matrix.shape}\")\n",
    "    print(f\"Matched: {hits}, Unmatched: {misses}\")\n",
    "\n",
    "    if missed_words:\n",
    "        sample_size = min(10, len(missed_words))\n",
    "        print(f\"Sample Unmatched: {random.sample(missed_words, sample_size)}\")\n",
    "\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "\n",
    "# Create embedding matrix if vocab exists\n",
    "if 'vocab' in locals():\n",
    "    embedding_matrix = create_embedding_matrix(vocab, EMB_DIM)\n",
    "else:\n",
    "    print(\"ERROR: 'vocab' not found. Please run Step 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQgH96Lljf_K"
   },
   "source": [
    "## RNN Classifier Architecture\n",
    "\n",
    "Defines a modular RNN-based classifier supporting both bi-directional LSTM and GRU architectures:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBG7wQJXjf_L",
    "outputId": "523991e3-d72f-4730-8ec9-9d6ddc0d99f1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    \"\"\"Bidirectional RNN classifier with configurable LSTM/GRU and pretrained embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 n_layers, bidirectional, dropout, pad_idx, rnn_type='LSTM', pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer: use pretrained if provided\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False, padding_idx=pad_idx)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # RNN layer: LSTM or GRU\n",
    "        self.rnn_type = rnn_type\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim,\n",
    "                               hidden_dim,\n",
    "                               num_layers=n_layers,\n",
    "                               bidirectional=bidirectional,\n",
    "                               dropout=dropout,\n",
    "                               batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim,\n",
    "                              hidden_dim,\n",
    "                              num_layers=n_layers,\n",
    "                              bidirectional=bidirectional,\n",
    "                              dropout=dropout,\n",
    "                              batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"rnn_type must be 'LSTM' or 'GRU'\")\n",
    "\n",
    "        # Output layer: adjust for bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        # Embed and apply dropout\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        # Pack sequences for efficient RNN processing\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Forward through RNN\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        else:\n",
    "            packed_output, hidden = self.rnn(packed_embedded)\n",
    "\n",
    "        # Concatenate final hidden states for bidirectional\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "\n",
    "        return self.fc(hidden)\n",
    "\n",
    "print(\"Model class UPDATED (GloVe supported).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mr8a10v0jf_L"
   },
   "source": [
    "## Training Utilities\n",
    "\n",
    "Defines essential training and evaluation functions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTbML8Zejf_L",
    "outputId": "3e7500d0-4661-41a9-999e-dce8410d6d82"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"Calculates binary classification accuracy.\"\"\"\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\"Trains the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for text, label, lengths in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text, lengths).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = binary_accuracy(predictions, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"Evaluates the model on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, label, lengths in iterator:\n",
    "            predictions = model(text, lengths).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, label)\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "print(\"Train and Evaluate functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlCJsoQvjf_M"
   },
   "source": [
    "## GloVe Experiment Loop: LSTM vs GRU\n",
    "\n",
    "Trains and evaluates bidirectional LSTM and GRU models with GloVe embeddings:\n",
    "\n",
    "\n",
    "### Output:\n",
    "- Saves `best_model_LSTM.pt` and `best_model_GRU.pt`\n",
    "- Prints comparison table showing test accuracy and training time for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG1VwVtFjf_M",
    "outputId": "ede84b75-5b88-49a6-f0c1-0aa558801cd8"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Training configuration\n",
    "N_LAYERS = 2\n",
    "models_to_train = ['LSTM', 'GRU']\n",
    "results = {}\n",
    "convergence_history = {}  # Track validation accuracy per epoch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"EXPERIMENT STARTING: {models_to_train}\")\n",
    "print(f\"Hyperparameters: HID_DIM={HID_DIM}, Layers={N_LAYERS}, Dropout={DROPOUT}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train both LSTM and GRU models\n",
    "for rnn_type in models_to_train:\n",
    "    print(f\"\\n--- Training Starting: Bidirectional {rnn_type} + GloVe ---\")\n",
    "\n",
    "    # Initialize model with GloVe embeddings\n",
    "    model = RNNClassifier(len(vocab), EMB_DIM, HID_DIM, OUTPUT_DIM,\n",
    "                          N_LAYERS, bidirectional=True, dropout=DROPOUT,\n",
    "                          pad_idx=PAD_IDX, rnn_type=rnn_type,\n",
    "                          pretrained_embeddings=embedding_matrix).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "    best_valid_acc = 0.0\n",
    "    model_start_time = time.time()\n",
    "    epoch_accuracies = []  # Store accuracies for convergence plot\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_loader_glove, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, val_loader_glove, criterion)\n",
    "        \n",
    "        epoch_accuracies.append(valid_acc)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins = int((end_time - start_time) / 60)\n",
    "        epoch_secs = int((end_time - start_time) % 60)\n",
    "\n",
    "        # Save best model based on validation accuracy\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            torch.save(model.state_dict(), f'best_model_{rnn_type}.pt')\n",
    "            save_msg = \"--> Model Saved!\"\n",
    "        else:\n",
    "            save_msg = \"\"\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% {save_msg}')\n",
    "\n",
    "    total_time = time.time() - model_start_time\n",
    "    print(f\"\\n{rnn_type} Training Completed. Total Time: {int(total_time)} sec\")\n",
    "    \n",
    "    # Store convergence history for visualization\n",
    "    convergence_history[f'GloVe-{rnn_type}'] = epoch_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation utilities\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GLOVE MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "print(\"\\n[1] Checking if required variables exist...\")\n",
    "\n",
    "required_vars = [\n",
    "    'vocab', 'embedding_matrix', 'test_loader_glove', 'device',\n",
    "    'EMB_DIM', 'HID_DIM', 'OUTPUT_DIM', 'N_LAYERS',\n",
    "    'DROPOUT', 'PAD_IDX'\n",
    "]\n",
    "\n",
    "for var_name in required_vars:\n",
    "    try:\n",
    "        var = eval(var_name)\n",
    "        if var_name == 'embedding_matrix':\n",
    "            print(f\" âœ“ {var_name}: exists, shape = {var.shape}, dtype = {var.dtype}\")\n",
    "        elif var_name == 'vocab':\n",
    "            print(f\" âœ“ {var_name}: exists, size = {len(var)}\")\n",
    "        else:\n",
    "            print(f\" âœ“ {var_name}: exists\")\n",
    "    except NameError:\n",
    "        print(f\" âœ— {var_name}: NOT FOUND!\")\n",
    "\n",
    "\n",
    "print(\"\\n[2] Checking if model files exist...\")\n",
    "\n",
    "for model_file in ['best_model_LSTM.pt', 'best_model_GRU.pt']:\n",
    "    if os.path.exists(model_file):\n",
    "        size_mb = os.path.getsize(model_file) / (1024**2)\n",
    "        print(f\" âœ“ {model_file}: exists ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\" âœ— {model_file}: NOT FOUND!\")\n",
    "\n",
    "\n",
    "print(\"\\n[3] Loading LSTM model and checking architecture...\")\n",
    "\n",
    "try:\n",
    "    model_lstm = RNNClassifier(\n",
    "        len(vocab), EMB_DIM, HID_DIM, OUTPUT_DIM, N_LAYERS,\n",
    "        bidirectional=True,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=PAD_IDX,\n",
    "        rnn_type='LSTM',\n",
    "        pretrained_embeddings=embedding_matrix\n",
    "    ).to(device)\n",
    "\n",
    "    print(\" âœ“ Model created successfully\")\n",
    "    print(f\" âœ“ Embedding layer shape: {model_lstm.embedding.weight.shape}\")\n",
    "    print(f\" âœ“ Is embedding frozen? {not model_lstm.embedding.weight.requires_grad}\")\n",
    "\n",
    "    # Load saved weights\n",
    "    model_lstm.load_state_dict(torch.load('best_model_LSTM.pt', weights_only=False))\n",
    "    print(\" âœ“ Weights loaded successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" âœ— Error creating/loading model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n[4] Testing on first batch of test data...\")\n",
    "\n",
    "try:\n",
    "    model_lstm.eval()\n",
    "\n",
    "    for text, labels, lengths in test_loader_glove:\n",
    "        print(f\" â†’ Batch shape: text={text.shape}, labels={labels.shape}, lengths={lengths.shape}\")\n",
    "        print(f\" â†’ Sample text indices: {text[0][:10]}\")\n",
    "        print(f\" â†’ Sample label: {labels[0].item()}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model_lstm(text, lengths).squeeze(1)\n",
    "            print(f\" â†’ Raw predictions (first 5): {predictions[:5]}\")\n",
    "            sigmoid_preds = torch.sigmoid(predictions)\n",
    "            print(f\" â†’ After sigmoid (first 5): {sigmoid_preds[:5]}\")\n",
    "            rounded_preds = torch.round(sigmoid_preds)\n",
    "            print(f\" â†’ After rounding (first 5): {rounded_preds[:5]}\")\n",
    "\n",
    "            unique_preds = torch.unique(rounded_preds)\n",
    "            print(f\" â†’ Unique predictions in batch: {unique_preds}\")\n",
    "\n",
    "            if len(unique_preds) == 1:\n",
    "                print(f\" âš ï¸ WARNING: All predictions are {unique_preds[0].item()}!\")\n",
    "        break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" âœ— Error during inference: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "print(\"\\n[5] Running full test evaluation...\")\n",
    "\n",
    "glove_models = {\n",
    "    'Bi-LSTM + GloVe': ('best_model_LSTM.pt', 'LSTM'),\n",
    "    'Bi-GRU + GloVe': ('best_model_GRU.pt', 'GRU'),\n",
    "}\n",
    "\n",
    "for model_name, (model_path, rnn_type) in glove_models.items():\n",
    "    print(f\"\\n--- Evaluating: {model_name} ---\")\n",
    "\n",
    "    try:\n",
    "        # Initialize model with same architecture\n",
    "        model = RNNClassifier(\n",
    "            len(vocab), EMB_DIM, HID_DIM, OUTPUT_DIM, N_LAYERS,\n",
    "            bidirectional=True,\n",
    "            dropout=DROPOUT,\n",
    "            pad_idx=PAD_IDX,\n",
    "            rnn_type=rnn_type,\n",
    "            pretrained_embeddings=embedding_matrix\n",
    "        ).to(device)\n",
    "\n",
    "        model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "        model.eval()\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_raw_logits = []\n",
    "\n",
    "        # Evaluate on test set\n",
    "        with torch.no_grad():\n",
    "            for text, labels, lengths in tqdm(test_loader_glove, desc=\"Testing\"):\n",
    "                predictions = model(text, lengths).squeeze(1)\n",
    "\n",
    "                all_raw_logits.extend(predictions.cpu().numpy())\n",
    "                preds = torch.round(torch.sigmoid(predictions)).cpu().numpy()\n",
    "\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_raw_logits = np.array(all_raw_logits)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        print(\"\\nðŸ“Š Results:\")\n",
    "        print(f\" Test Accuracy: {accuracy*100:.2f}%\")\n",
    "        print(f\" Test Macro-F1: {macro_f1:.4f}\")\n",
    "\n",
    "        print(\"\\nðŸ“ˆ Prediction Distribution:\")\n",
    "        unique, counts = np.unique(all_preds, return_counts=True)\n",
    "        for v, c in zip(unique, counts):\n",
    "            print(f\" Predicted {int(v)}: {c} samples ({c/len(all_preds)*100:.1f}%)\")\n",
    "\n",
    "        print(\"\\nðŸ“ˆ True Label Distribution:\")\n",
    "        unique, counts = np.unique(all_labels, return_counts=True)\n",
    "        for v, c in zip(unique, counts):\n",
    "            print(f\" True {int(v)}: {c} samples ({c/len(all_labels)*100:.1f}%)\")\n",
    "\n",
    "        print(\"\\nðŸ“Š Raw Logits Statistics:\")\n",
    "        print(f\" Min: {all_raw_logits.min():.4f}\")\n",
    "        print(f\" Max: {all_raw_logits.max():.4f}\")\n",
    "        print(f\" Mean: {all_raw_logits.mean():.4f}\")\n",
    "        print(f\" Std: {all_raw_logits.std():.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" âœ— Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEBUG COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifczL5Tijf_M"
   },
   "source": [
    "## BERT Setup: Tokenizer and Data Loaders\n",
    "\n",
    "Prepares the infrastructure for BERT-based experiments (distil-bert-uncased)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjkRsDOvjf_M",
    "outputId": "6f1003e5-7b02-4faa-97ad-564f71dab560"
   },
   "outputs": [],
   "source": [
    "# Import BERT tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SECTION 2: PREPARING CONTEXTUAL EMBEDDINGS (DISTILBERT)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load DistilBERT tokenizer\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def collate_batch_bert(batch):\n",
    "    \"\"\"Custom collate function for BERT DataLoader.\"\"\"\n",
    "    text_list = [item['text'] for item in batch]\n",
    "    label_list = [float(item['label']) for item in batch]\n",
    "\n",
    "    # Tokenize with BERT tokenizer\n",
    "    encodings = bert_tokenizer(text_list,\n",
    "                                truncation=True,\n",
    "                                padding=True,\n",
    "                                max_length=256,\n",
    "                                return_tensors='pt')\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.float).to(device)\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "# Create DataLoaders for BERT path\n",
    "BERT_BATCH_SIZE = 32\n",
    "\n",
    "train_loader_bert = DataLoader(dataset['train'], batch_size=BERT_BATCH_SIZE, shuffle=True, collate_fn=collate_batch_bert)\n",
    "val_loader_bert = DataLoader(dataset['validation'], batch_size=BERT_BATCH_SIZE, shuffle=False, collate_fn=collate_batch_bert)\n",
    "test_loader_bert = DataLoader(dataset['test'], batch_size=BERT_BATCH_SIZE, shuffle=False, collate_fn=collate_batch_bert)\n",
    "\n",
    "print(f\"DistilBERT DataLoaders ready. Batch Size: {BERT_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htZ2PC5Zjf_M"
   },
   "source": [
    "## BERT-RNN Classifier Architecture\n",
    "\n",
    "Defines a hybrid architecture combining frozen BERT with trainable RNN:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBe0-Sxjjf_M",
    "outputId": "3b72905c-5116-4804-cec0-42a6f12e97af"
   },
   "outputs": [],
   "source": [
    "# Import BERT model\n",
    "from transformers import AutoModel\n",
    "\n",
    "class BERTRNNClassifier(nn.Module):\n",
    "    \"\"\"Hybrid classifier: frozen BERT encoder + trainable bidirectional RNN.\"\"\"\n",
    "    \n",
    "    def __init__(self, rnn_type, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pretrained BERT and freeze parameters\n",
    "        self.bert = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        embedding_dim = self.bert.config.hidden_size\n",
    "\n",
    "        # RNN layer on top of BERT\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                               bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                              bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT embeddings (frozen)\n",
    "        with torch.no_grad():\n",
    "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        embedded = self.dropout(bert_output.last_hidden_state)\n",
    "\n",
    "        # Forward through RNN\n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            output, (hidden, cell) = self.rnn(embedded)\n",
    "        else:\n",
    "            output, hidden = self.rnn(embedded)\n",
    "\n",
    "        # Concatenate bidirectional hidden states\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "\n",
    "        return self.fc(hidden)\n",
    "\n",
    "print(\"DistilBERT-RNN Architecture Defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_3gdN7cjf_N"
   },
   "source": [
    "## BERT Experiment Loop: LSTM vs GRU\n",
    "\n",
    "Trains and evaluates BERT-based models with LSTM and GRU heads:\n",
    "\n",
    "### Output Format:\n",
    "- Saves best models: `best_bert_LSTM.pt` and `best_bert_GRU.pt`\n",
    "- Based on validation accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8n-5rg83jf_N",
    "outputId": "fae1aeeb-2565-4040-b011-2295a39fd940"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Training functions for BERT models\n",
    "def train_bert(model, iterator, optimizer, criterion):\n",
    "    \"\"\"Trains BERT model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for input_ids, mask, label in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(input_ids, mask).squeeze(1)\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = binary_accuracy(predictions, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_bert(model, iterator, criterion):\n",
    "    \"\"\"Evaluates BERT model on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, mask, label in iterator:\n",
    "            predictions = model(input_ids, mask).squeeze(1)\n",
    "            loss = criterion(predictions, label)\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# Train both LSTM and GRU with BERT embeddings\n",
    "bert_models = ['LSTM', 'GRU']\n",
    "\n",
    "print(f\"\\nDistilBERT EXPERIMENT STARTING: {bert_models}\")\n",
    "\n",
    "for rnn_type in bert_models:\n",
    "    print(f\"\\n--- Training Starting: DistilBERT + Bi-{rnn_type} ---\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = BERTRNNClassifier(rnn_type, HID_DIM, OUTPUT_DIM, N_LAYERS, True, DROPOUT).to(device)\n",
    "\n",
    "    # Only optimize RNN parameters (BERT is frozen)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "    best_valid_acc = 0.0\n",
    "    epoch_accuracies = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train_bert(model, train_loader_bert, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate_bert(model, val_loader_bert, criterion)\n",
    "        \n",
    "        epoch_accuracies.append(valid_acc)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins = int((end_time - start_time) / 60)\n",
    "        epoch_secs = int((end_time - start_time) % 60)\n",
    "\n",
    "        # Save best model\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            torch.save(model.state_dict(), f'best_bert_{rnn_type}.pt')\n",
    "            save_msg = \"--> Saved\"\n",
    "        else:\n",
    "            save_msg = \"\"\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% {save_msg}')\n",
    "    \n",
    "    # Store for convergence visualization\n",
    "    convergence_history[f'BERT-{rnn_type}'] = epoch_accuracies\n",
    "\n",
    "print(\"\\nBERT Trainings Completed. Models saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qw2peCfEjf_N"
   },
   "source": [
    "## Final Evaluation: Comprehensive Metrics\n",
    "\n",
    "Performs rigorous evaluation of all four trained models on the test set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sej4moWyjf_N"
   },
   "outputs": [],
   "source": [
    "# Import evaluation and visualization libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION AND PLOTTING OF ALL MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Storage for final results\n",
    "final_metrics = {\n",
    "    'Model': [],\n",
    "    'Accuracy': [],\n",
    "    'Macro-F1': []\n",
    "}\n",
    "\n",
    "# Define all models to evaluate\n",
    "models_to_evaluate = {\n",
    "    'Bi-LSTM + GloVe': ('best_model_LSTM.pt', RNNClassifier, 'LSTM', False),\n",
    "    'Bi-GRU + GloVe':  ('best_model_GRU.pt',  RNNClassifier, 'GRU',  False),\n",
    "    'DistilBERT + Bi-LSTM': ('best_bert_LSTM.pt', BERTRNNClassifier, 'LSTM', True),\n",
    "    'DistilBERT + Bi-GRU':  ('best_bert_GRU.pt',  BERTRNNClassifier, 'GRU',  True),\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, (model_path, model_class, rnn_type, is_bert_model) in models_to_evaluate.items():\n",
    "    print(f\"Evaluating: {model_name}...\")\n",
    "\n",
    "    # Initialize model with appropriate architecture\n",
    "    if is_bert_model:\n",
    "        model = model_class(rnn_type, HID_DIM, OUTPUT_DIM, N_LAYERS, True, DROPOUT).to(device)\n",
    "        current_loader = test_loader_bert\n",
    "    else:\n",
    "        model = model_class(len(vocab), EMB_DIM, HID_DIM, OUTPUT_DIM, \n",
    "                            N_LAYERS, bidirectional=True, dropout=DROPOUT, \n",
    "                            pad_idx=PAD_IDX, rnn_type=rnn_type, \n",
    "                            pretrained_embeddings=embedding_matrix).to(device)\n",
    "        current_loader = test_loader_glove\n",
    "    \n",
    "    # Load saved weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Run inference on test set\n",
    "    with torch.no_grad():\n",
    "        for batch in current_loader:\n",
    "            if is_bert_model:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                predictions = model(input_ids, attention_mask).squeeze(1)\n",
    "            else:\n",
    "                text, labels, lengths = batch\n",
    "                predictions = model(text, lengths).squeeze(1)\n",
    "            \n",
    "            preds = torch.round(torch.sigmoid(predictions)).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # Store results\n",
    "    final_metrics['Model'].append(model_name)\n",
    "    final_metrics['Accuracy'].append(acc)\n",
    "    final_metrics['Macro-F1'].append(f1)\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(final_metrics)\n",
    "print(\"\\n Output Table:\")\n",
    "print(df_results)\n",
    "\n",
    "# Visualization: comparison bar plot\n",
    "df_melted = df_results.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=\"Model\", y=\"Score\", hue=\"Metric\", data=df_melted, palette=\"viridis\")\n",
    "\n",
    "# Add value labels on bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.4f', padding=3, fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.title('Model Performance Comparison (Auto-Generated)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Set y-axis limits for better visualization\n",
    "min_score = df_melted['Score'].min()\n",
    "plt.ylim(min_score - 0.02, min_score + 0.05) \n",
    "\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.xlabel('Architecture', fontsize=12)\n",
    "plt.legend(loc='lower right', title='Metric')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_comparison_auto.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Efficiency Analysis\n",
    "\n",
    "Visualizes how quickly each model reaches the accuracy threshold (0.87):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Threshold for convergence analysis\n",
    "THRESHOLD = 0.87\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Color palette: Blues for GloVe, Oranges for BERT\n",
    "colors = {\n",
    "    'GloVe-LSTM': '#1f77b4',  # Dark Blue\n",
    "    'GloVe-GRU':  '#6baed6',  # Light Blue\n",
    "    'BERT-LSTM':  '#ff7f0e',  # Dark Orange\n",
    "    'BERT-GRU':   '#fdae6b'   # Light Orange\n",
    "}\n",
    "\n",
    "# Plot convergence curves for all models\n",
    "for model_name, accuracies in convergence_history.items():\n",
    "    epochs = range(1, len(accuracies) + 1)\n",
    "    \n",
    "    # Line plot with markers\n",
    "    plt.plot(epochs, accuracies, label=model_name, color=colors.get(model_name, 'black'), linewidth=2.5, marker='o')\n",
    "    \n",
    "    # Mark first epoch where threshold is reached\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        if acc >= THRESHOLD:\n",
    "            plt.plot(i+1, acc, marker='*', color='red', markersize=15, zorder=10)\n",
    "            plt.text(i+1, acc+0.005, f\"{i+1}. Epoch\", color='red', fontsize=9, fontweight='bold')\n",
    "            break\n",
    "\n",
    "# Threshold line\n",
    "plt.axhline(y=THRESHOLD, color='r', linestyle='--', linewidth=2, alpha=0.7, label=f'Threshold ({THRESHOLD})')\n",
    "\n",
    "plt.title('Convergence Efficiency: Validation Accuracy per Epoch', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Validation Accuracy', fontsize=14)\n",
    "plt.ylim(0.80, 0.95) \n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('convergence_efficiency.png', dpi=300)\n",
    "plt.show()\n",
    "print(\"Plot saved as 'convergence_efficiency.png'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3RtAnrWjf_N"
   },
   "source": [
    "## Latent Space Visualization: PCA & t-SNE\n",
    "\n",
    "Visualizes and compares the learned representations of all four models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEzfuFgqjf_N"
   },
   "outputs": [],
   "source": [
    "# Import dimensionality reduction and visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Visualization parameters\n",
    "N_SAMPLES = 1000\n",
    "PERPLEXITY = 30\n",
    "SEED = 1234\n",
    "\n",
    "# Custom color palette: Blue for negative, Orange for positive\n",
    "CUSTOM_PALETTE = [\"#1f77b4\", \"#ff7f0e\"]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LATENT SPACE VISUALIZATION (BLUE/ORANGE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create fresh DataLoaders for visualization\n",
    "try:\n",
    "    viz_loader_glove = DataLoader(dataset['test'], batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "    print(\"  GloVe Loader refreshed.\")\n",
    "except NameError:\n",
    "    print(\"ERROR: 'collate_batch' not found. Run Step 2.\")\n",
    "\n",
    "try:\n",
    "    viz_loader_bert = DataLoader(dataset['test'], batch_size=32, shuffle=True, collate_fn=collate_batch_bert)\n",
    "    print(\"  BERT Loader refreshed.\")\n",
    "except NameError:\n",
    "    print(\"ERROR: 'collate_batch_bert' not found. Run Cell 7.\")\n",
    "\n",
    "# Configuration for all models\n",
    "viz_configs = [\n",
    "    (\"GloVe-LSTM\", \"best_model_LSTM.pt\", \"RNN\", viz_loader_glove, False),\n",
    "    (\"GloVe-GRU\",  \"best_model_GRU.pt\",  \"RNN\", viz_loader_glove, False),\n",
    "    (\"BERT-LSTM\",  \"best_bert_LSTM.pt\",  \"BERT\", viz_loader_bert, True),\n",
    "    (\"BERT-GRU\",   \"best_bert_GRU.pt\",   \"BERT\", viz_loader_bert, True)\n",
    "]\n",
    "\n",
    "def extract_features_robust(model, loader, is_bert, model_name):\n",
    "    \"\"\"Extracts latent representations from RNN hidden states.\"\"\"\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if total_count >= N_SAMPLES: break\n",
    "\n",
    "            if is_bert:\n",
    "                # BERT path: get hidden states from BERT then RNN\n",
    "                inputs, mask, targets = batch\n",
    "                bert_out = model.bert(inputs, mask)\n",
    "                embedded = model.dropout(bert_out.last_hidden_state)\n",
    "                if \"LSTM\" in model_name:\n",
    "                    _, (hidden, _) = model.rnn(embedded)\n",
    "                else:\n",
    "                    _, hidden = model.rnn(embedded)\n",
    "            else:\n",
    "                # GloVe path: embed then RNN\n",
    "                text, targets, lengths = batch\n",
    "                embedded = model.dropout(model.embedding(text))\n",
    "                packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "                    embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "                )\n",
    "                if \"LSTM\" in model_name:\n",
    "                    _, (hidden, _) = model.rnn(packed_embedded)\n",
    "                else:\n",
    "                    _, hidden = model.rnn(packed_embedded)\n",
    "\n",
    "            # Concatenate bidirectional hidden states\n",
    "            if model.rnn.bidirectional:\n",
    "                latent_vec = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "            else:\n",
    "                latent_vec = hidden[-1,:,:]\n",
    "\n",
    "            features_list.append(latent_vec.cpu())\n",
    "            labels_list.append(targets.cpu())\n",
    "            total_count += len(targets)\n",
    "\n",
    "    if len(features_list) > 0:\n",
    "        features = torch.cat(features_list, dim=0)[:N_SAMPLES].numpy()\n",
    "        labels = torch.cat(labels_list, dim=0)[:N_SAMPLES].numpy()\n",
    "        print(f\"  -> {model_name}: {len(features)} samples successfully extracted.\")\n",
    "        return features, labels\n",
    "    else:\n",
    "        print(f\"  -> ERROR: No data could be extracted for {model_name}!\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "# Create 2x4 subplot grid: PCA on top, t-SNE on bottom\n",
    "fig, axes = plt.subplots(2, 4, figsize=(24, 12))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "print(\"\\nFetching and processing data...\")\n",
    "\n",
    "for idx, (name, path, m_type, loader, is_bert) in enumerate(viz_configs):\n",
    "\n",
    "    try:\n",
    "        # Load model\n",
    "        if m_type == \"RNN\":\n",
    "            model = RNNClassifier(len(vocab), EMB_DIM, HID_DIM, OUTPUT_DIM,\n",
    "                                  N_LAYERS, True, DROPOUT, PAD_IDX,\n",
    "                                  rnn_type=name.split('-')[1], pretrained_embeddings=None).to(device)\n",
    "        else:\n",
    "            model = BERTRNNClassifier(name.split('-')[1], HID_DIM, OUTPUT_DIM,\n",
    "                                      N_LAYERS, True, DROPOUT).to(device)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "    except Exception as e:\n",
    "        print(f\"Model Loading Error ({name}): {e}\")\n",
    "        continue\n",
    "\n",
    "    # Extract features\n",
    "    feats, y = extract_features_robust(model, loader, is_bert, name)\n",
    "\n",
    "    if len(feats) == 0: continue\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    feats_norm = scaler.fit_transform(feats)\n",
    "\n",
    "    # PCA reduction\n",
    "    pca = PCA(n_components=2, random_state=SEED)\n",
    "    pca_res = pca.fit_transform(feats_norm)\n",
    "\n",
    "    # Plot PCA\n",
    "    sns.scatterplot(x=pca_res[:,0], y=pca_res[:,1], hue=y, palette=CUSTOM_PALETTE,\n",
    "                    s=40, alpha=0.7, ax=axes[0, idx], legend=False, linewidth=0)\n",
    "    axes[0, idx].set_title(f\"{name} - PCA\")\n",
    "    axes[0, idx].set_xticks([])\n",
    "    axes[0, idx].set_yticks([])\n",
    "    if idx == 0: axes[0, idx].set_ylabel(\"PCA\", fontsize=12, fontweight='bold')\n",
    "\n",
    "    # t-SNE reduction\n",
    "    tsne = TSNE(n_components=2, random_state=SEED, perplexity=PERPLEXITY, n_iter=1000)\n",
    "    tsne_res = tsne.fit_transform(feats_norm)\n",
    "\n",
    "    # Plot t-SNE\n",
    "    sns.scatterplot(x=tsne_res[:,0], y=tsne_res[:,1], hue=y, palette=CUSTOM_PALETTE,\n",
    "                    s=40, alpha=0.7, ax=axes[1, idx], legend=(idx==3), linewidth=0)\n",
    "    axes[1, idx].set_title(f\"{name} - t-SNE\")\n",
    "    axes[1, idx].set_xticks([])\n",
    "    axes[1, idx].set_yticks([])\n",
    "\n",
    "# Add legend to last subplot\n",
    "handles, _ = axes[1, 3].get_legend_handles_labels()\n",
    "if handles:\n",
    "    axes[1, 3].legend(handles, [\"Negative (Blue)\", \"Positive (Orange)\"], title=\"Sentiment\", loc='upper right')\n",
    "\n",
    "plt.suptitle(\"Latent Space: GloVe vs BERT (Blue/Orange Separation)\", fontsize=18, y=0.95)\n",
    "plt.savefig(\"latent_space_blue_orange.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved as 'latent_space_blue_orange.png'.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
